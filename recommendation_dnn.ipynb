{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":918},"id":"ZZKj7TXXl6Iz","outputId":"470412e4-8787-4644-8fac-b36d13d20581"},"outputs":[{"name":"stdout","output_type":"stream","text":["['aisles.csv', 'departments.csv', 'orders.csv', 'order_products__prior.csv', 'order_products__train.csv', 'products.csv', 'sample_submission.csv', 'user_order_amount_matrix.csv']\n","read aisles.csv, shape: (134, 2)\n","read departments.csv, shape: (21, 2)\n","read orders.csv, shape: (3421083, 7)\n","read order_products__prior.csv, shape: (32434489, 4)\n","read order_products__train.csv, shape: (1384617, 4)\n","read products.csv, shape: (49688, 4)\n","read sample_submission.csv, shape: (75000, 2)\n","\n"," sample DataFrame (aisles.csv):\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>aisle_id</th>\n","      <th>aisle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>prepared soups salads</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>specialty cheeses</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>energy granola bars</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>instant foods</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>marinades meat preparation</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>129</th>\n","      <td>130</td>\n","      <td>hot cereal pancake mixes</td>\n","    </tr>\n","    <tr>\n","      <th>130</th>\n","      <td>131</td>\n","      <td>dry pasta</td>\n","    </tr>\n","    <tr>\n","      <th>131</th>\n","      <td>132</td>\n","      <td>beauty</td>\n","    </tr>\n","    <tr>\n","      <th>132</th>\n","      <td>133</td>\n","      <td>muscles joints pain relief</td>\n","    </tr>\n","    <tr>\n","      <th>133</th>\n","      <td>134</td>\n","      <td>specialty wines champagnes</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>134 rows × 2 columns</p>\n","</div>"],"text/plain":["     aisle_id                       aisle\n","0           1       prepared soups salads\n","1           2           specialty cheeses\n","2           3         energy granola bars\n","3           4               instant foods\n","4           5  marinades meat preparation\n","..        ...                         ...\n","129       130    hot cereal pancake mixes\n","130       131                   dry pasta\n","131       132                      beauty\n","132       133  muscles joints pain relief\n","133       134  specialty wines champagnes\n","\n","[134 rows x 2 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["import os\n","import pandas as pd\n","\n","directory = r\"C:\\Users\\15801\\OneDrive\\桌面\\NU\\2025winter\\data mining\\project\\instacart-market-basket-analysis\"\n","\n","all_files = os.listdir(directory)\n","\n","csv_files = [f for f in all_files if f.endswith(\".csv\")]\n","print(csv_files)\n","dataframes = {}\n","\n","for file in csv_files:\n","    if file not in 'user_order_amount_matrix.csv':\n","        file_path = os.path.join(directory, file, file)\n","        df = pd.read_csv(file_path)\n","        dataframes[file] = df\n","        print(f\"read {file}, shape: {df.shape}\")\n","\n","first_file = csv_files[0] if csv_files else None\n","if first_file:\n","    print(f\"\\n sample DataFrame ({first_file}):\")\n","    display(dataframes[first_file])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgVvmfMK8Xz6"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","\n","# only using order_data.csv\n","order_df = dataframes[\"orders.csv\"]\n","aisles_df = dataframes[\"aisles.csv\"]\n","department_df = dataframes[\"departments.csv\"]\n","train_df = dataframes[\"order_products__prior.csv\"]\n","train_df = order_df.merge(train_df, on=\"order_id\", how=\"inner\")\n","\n","train_df = train_df.drop_duplicates(subset=['user_id', 'order_id'], keep=\"first\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KFXwVGMi5da6","outputId":"c2ceb756-e36a-4ff6-923c-4d4a2fbdbdd8"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>order_id</th>\n","      <th>user_id</th>\n","      <th>eval_set</th>\n","      <th>order_number</th>\n","      <th>order_dow</th>\n","      <th>order_hour_of_day</th>\n","      <th>days_since_prior_order</th>\n","      <th>order_amount_scaled</th>\n","      <th>product_id</th>\n","      <th>add_to_cart_order</th>\n","      <th>reordered</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1187899</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>14.0</td>\n","      <td>-0.366918</td>\n","      <td>196</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1187899</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>14.0</td>\n","      <td>-0.366918</td>\n","      <td>25133</td>\n","      <td>2</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1187899</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>14.0</td>\n","      <td>-0.366918</td>\n","      <td>38928</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1187899</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>14.0</td>\n","      <td>-0.366918</td>\n","      <td>26405</td>\n","      <td>4</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1187899</td>\n","      <td>1</td>\n","      <td>train</td>\n","      <td>11</td>\n","      <td>4</td>\n","      <td>8</td>\n","      <td>14.0</td>\n","      <td>-0.366918</td>\n","      <td>39657</td>\n","      <td>5</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n","0   1187899        1    train            11          4                  8   \n","1   1187899        1    train            11          4                  8   \n","2   1187899        1    train            11          4                  8   \n","3   1187899        1    train            11          4                  8   \n","4   1187899        1    train            11          4                  8   \n","\n","   days_since_prior_order  order_amount_scaled  product_id  add_to_cart_order  \\\n","0                    14.0            -0.366918         196                  1   \n","1                    14.0            -0.366918       25133                  2   \n","2                    14.0            -0.366918       38928                  3   \n","3                    14.0            -0.366918       26405                  4   \n","4                    14.0            -0.366918       39657                  5   \n","\n","   reordered  \n","0          1  \n","1          1  \n","2          1  \n","3          1  \n","4          1  "]},"execution_count":56,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J_yN_Zua5da7","outputId":"7bc1189f-36ea-4b66-8371-55c0aa3e4243"},"outputs":[{"name":"stdout","output_type":"stream","text":["(32434489, 10)\n","(32434489, 10)\n","(30356421, 10)\n"]}],"source":["order_df = dataframes[\"orders.csv\"]\n","aisles_df = dataframes[\"aisles.csv\"]\n","department_df = dataframes[\"departments.csv\"]\n","train_df = dataframes[\"order_products__prior.csv\"]\n","train_df = order_df.merge(train_df, on=\"order_id\", how=\"inner\")\n","print(train_df.shape)\n","train_df = train_df.drop_duplicates(subset=['user_id', 'order_id', 'product_id'], keep=\"first\")\n","print(train_df.shape)\n","train_df = train_df.dropna()\n","print(train_df.shape)\n","product_sales = train_df.groupby(\"product_id\")[\"order_id\"].count().reset_index()\n","product_sales.rename(columns={\"order_id\": \"total_sales\"}, inplace=True)\n","product_sales = product_sales.sort_values(by=\"total_sales\", ascending=False)\n","\n","popular_products = product_sales[product_sales[\"total_sales\"] >= 500][\"product_id\"]\n","\n","user_order_counts = train_df.groupby(\"user_id\")[\"order_id\"].nunique().reset_index()\n","user_order_counts.rename(columns={\"order_id\": \"total_orders\"}, inplace=True)\n","active_users = user_order_counts[user_order_counts[\"total_orders\"] >= 15][\"user_id\"]\n","\n","train_df = train_df[(train_df[\"product_id\"].isin(popular_products)) & (train_df[\"user_id\"].isin(active_users))]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LfBfqBaU5da7","outputId":"4a8199a9-d068-4e44-adda-becf9e939988"},"outputs":[{"name":"stdout","output_type":"stream","text":["     order_id  user_id eval_set  order_number  order_dow  order_hour_of_day  \\\n","59    2168274        2    prior             1          2                 11   \n","60    2168274        2    prior             1          2                 11   \n","61    2168274        2    prior             1          2                 11   \n","62    2168274        2    prior             1          2                 11   \n","63    2168274        2    prior             1          2                 11   \n","..        ...      ...      ...           ...        ...                ...   \n","249    839880        2    prior            14          3                 10   \n","250    839880        2    prior            14          3                 10   \n","251    839880        2    prior            14          3                 10   \n","252    839880        2    prior            14          3                 10   \n","253    839880        2    prior            14          3                 10   \n","\n","     days_since_prior_order  order_amount_scaled  product_id  \\\n","59                      NaN            -0.930676       32792   \n","60                      NaN            -0.930676       47766   \n","61                      NaN            -0.930676       20574   \n","62                      NaN            -0.930676       12000   \n","63                      NaN            -0.930676       48110   \n","..                      ...                  ...         ...   \n","249                    13.0            -0.125351        2573   \n","250                    13.0            -0.125351       39928   \n","251                    13.0            -0.125351       20785   \n","252                    13.0            -0.125351       24768   \n","253                    13.0            -0.125351        7963   \n","\n","     add_to_cart_order  reordered  \n","59                   1          0  \n","60                   2          0  \n","61                   3          0  \n","62                   4          0  \n","63                   5          0  \n","..                 ...        ...  \n","249                 12          1  \n","250                 13          0  \n","251                 14          0  \n","252                 15          0  \n","253                 16          0  \n","\n","[195 rows x 11 columns]\n"]},{"data":{"text/plain":["array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n","       18, 19, 20, 21, 22, 23, 24, 25, 26], dtype=int64)"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["train_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hC2pt1_w5da7","outputId":"45706cee-cf86-4c3e-a6dc-11a6cdfda50d"},"outputs":[{"name":"stdout","output_type":"stream","text":["(19418203, 10)\n","65290\n","7907\n"]}],"source":["print(train_df.shape)\n","print(len(train_df[\"user_id\"].unique()))\n","print(len(train_df[\"product_id\"].unique()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2rPyQd25da8","outputId":"299788f1-de81-44b0-d4d6-a919cf930f77"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished order_hour_scaler\n","days_since_prior_order_scaled finished\n"]}],"source":["from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","\n","def df_process(df):\n","  scaler = StandardScaler()\n","  df[['order_hour_of_day', 'order_dow', 'days_since_prior_order', \"order_number\"]] = scaler.fit_transform(\n","      df[['order_hour_of_day', 'order_dow', 'days_since_prior_order', \"order_number\"]]\n","  )\n","  print(\"Finished order_hour_scaler\")\n","\n","  \"\"\"\n","  df['order_amount_scaled'] = 0\n","  scalers={}\n","  for product in df[\"product_id\"].unique():\n","    scaler = StandardScaler()\n","    product_mask = df['product_id'] == product\n","    df.loc[product_mask, 'order_amount_scaled'] = scaler.fit_transform(df.loc[product_mask, ['order_number']])\n","    scalers[product] = scaler\n","  print(\"Finished product count scaler\")\n","  \"\"\"\n","\n","  minMaxScaler = MinMaxScaler(feature_range=(0, 1))\n","  df[['days_since_prior_order_scaled']] = minMaxScaler.fit_transform(\n","      df[['days_since_prior_order']]\n","  )\n","  print(\"days_since_prior_order_scaled finished\")\n","\n","  df[['days_since_prior_order_scaled']] = 1 - df[['days_since_prior_order_scaled']]\n","  df['weighted_order_amount'] = df['order_number'] * df['days_since_prior_order_scaled']\n","  return df\n","\n","train_df = df_process(train_df)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V6EwwaIa5da8"},"outputs":[],"source":["np.random.seed(42)\n","users_sample = np.random.choice(user_order_amount_matrix.index.unique(), 1000, replace=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TXGFH2F5da9","outputId":"4f7e78e0-6fe3-43ac-d9a4-3cfc0b41154a"},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch Version: 2.6.0+cu118\n","CUDA Available: True\n","Number of GPUs: 1\n","GPU Name: NVIDIA GeForce RTX 3070 Ti Laptop GPU\n"]}],"source":["import torch\n","print(\"PyTorch Version:\", torch.__version__)\n","print(\"CUDA Available:\", torch.cuda.is_available())\n","print(\"Number of GPUs:\", torch.cuda.device_count())\n","print(\"GPU Name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c-Nkb7VP5da9","outputId":"4cd895cd-965c-4bd2-c96e-72117e81c45f"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n","Epoch 1, Loss: 0.4342\n","Epoch 2, Loss: 0.4115\n","Epoch 3, Loss: 0.4028\n","Epoch 4, Loss: 0.3948\n","Epoch 5, Loss: 0.3879\n","Test RMSE: 0.6336\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","import numpy as np\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","train_df = train_df[['user_id', 'product_id', 'weighted_order_amount']].dropna()\n","\n","# Encode user_id and product_id to unique indices\n","user_mapping = {user: idx for idx, user in enumerate(train_df[\"user_id\"].unique())}\n","product_mapping = {product: idx for idx, product in enumerate(train_df[\"product_id\"].unique())}\n","\n","train_df[\"user_id\"] = train_df[\"user_id\"].map(user_mapping)\n","train_df[\"product_id\"] = train_df[\"product_id\"].map(product_mapping)\n","\n","# Split into train and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    train_df[[\"user_id\", \"product_id\"]].values, train_df[\"weighted_order_amount\"].values,\n","    test_size=0.2, random_state=42\n",")\n","\n","# Convert to tensors\n","X_train_user = torch.tensor(X_train[:, 0], dtype=torch.long)\n","X_train_product = torch.tensor(X_train[:, 1], dtype=torch.long)\n","y_train = torch.tensor(y_train, dtype=torch.float32)\n","\n","X_test_user = torch.tensor(X_test[:, 0], dtype=torch.long)\n","X_test_product = torch.tensor(X_test[:, 1], dtype=torch.long)\n","y_test = torch.tensor(y_test, dtype=torch.float32)\n","\n","# Create Dataset Class\n","class OrderDataset(Dataset):\n","    def __init__(self, users, products, ratings):\n","        self.users = users\n","        self.products = products\n","        self.ratings = ratings\n","\n","    def __len__(self):\n","        return len(self.users)\n","\n","    def __getitem__(self, idx):\n","        return self.users[idx], self.products[idx], self.ratings[idx]\n","\n","# Create DataLoaders\n","train_dataset = OrderDataset(X_train_user, X_train_product, y_train)\n","test_dataset = OrderDataset(X_test_user, X_test_product, y_test)\n","\n","train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n","test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n","\n","# Define Neural Collaborative Filtering Model\n","class NCF(nn.Module):\n","    def __init__(self, num_users, num_products, embedding_dim=64):\n","        super(NCF, self).__init__()\n","        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n","        self.product_embedding = nn.Embedding(num_products, embedding_dim)\n","\n","        self.fc1 = nn.Linear(embedding_dim * 2, 128)\n","        self.fc2 = nn.Linear(128, 64)\n","        self.fc3 = nn.Linear(64, 1)\n","\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, user, product):\n","        user_emb = self.user_embedding(user)\n","        product_emb = self.product_embedding(product)\n","        x = torch.cat([user_emb, product_emb], dim=-1)\n","        x = self.relu(self.fc1(x))\n","        x = self.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x.squeeze()\n","\n","# Initialize Model\n","num_users = len(user_mapping)\n","num_products = len(product_mapping)\n","model = NCF(num_users, num_products).to(device)\n","\n","# Define Loss and Optimizer\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training Loop\n","epochs = 5\n","for epoch in range(epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for users, products, ratings in train_loader:\n","        users = users.to(device)\n","        products = products.to(device)\n","        ratings = ratings.to(device)\n","\n","        optimizer.zero_grad()\n","        predictions = model(users, products)\n","        loss = criterion(predictions, ratings)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {running_loss / len(train_loader):.4f}\")\n","\n","# Evaluation\n","model.eval()\n","predictions_list = []\n","actual_list = []\n","\n","with torch.no_grad():\n","    for users, products, ratings in test_loader:\n","        users, products, ratings = users.to(device), products.to(device), ratings.to(device)\n","        predictions = model(users, products)\n","        predictions_list.extend(predictions.cpu().numpy())\n","        actual_list.extend(ratings.cpu().numpy())\n","\n","rmse = np.sqrt(mean_squared_error(actual_list, predictions_list))\n","print(f\"Test RMSE: {rmse:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t417YP_y5da9","outputId":"71f86438-44db-401d-dd3e-9abb3420c01b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Test RMSE: 0.6336\n"]}],"source":["rmse = np.sqrt(mean_squared_error(actual_list, predictions_list))\n","print(f\"Test RMSE: {rmse:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fsfjDtof5da9"},"outputs":[],"source":["torch.save(model, \"ncf_model.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4QWiNdT5dbG"},"outputs":[],"source":["from sklearn.metrics import ndcg_score\n","\n","users_sample = np.random.choice(user_order_amount_matrix.index.unique(), 1000, replace=False)\n","\n","def evaluate_metrics(user_id, K=5):\n","    user_id = int(user_id)\n","\n","    if user_id not in user_mapping.values():\n","        return None, None\n","\n","    true_orders = np.zeros(num_products)\n","    user_purchases = train_df[train_df[\"user_id\"] == user_id][\"product_id\"].values\n","    for product in user_purchases:\n","        true_orders[product] = 1\n","\n","    recommendations = model(torch.tensor([user_id]*K).to(device), torch.tensor(range(K)).to(device))\n","    recommended_indices = np.argsort(recommendations.cpu().detach().numpy())[::-1][:K]  # 取Top-K推荐\n","\n","    hr_at_k = int(any(true_orders[i] for i in recommended_indices))\n","\n","    ideal_dcg = sum([1.0 / np.log2(i + 2) for i in range(min(K, int(true_orders.sum())))])\n","    dcg = sum([1.0 / np.log2(i + 2) if true_orders[i] > 0 else 0 for i in recommended_indices])\n","    ndcg_at_k = dcg / ideal_dcg if ideal_dcg > 0 else 0\n","\n","    return hr_at_k, ndcg_at_k\n","\n","K_values = list(range(1, 21))\n","hr_scores = []\n","ndcg_scores = []\n","\n","for K in K_values:\n","    hr_list = []\n","    ndcg_list = []\n","\n","    for user in users_sample:\n","        hr, ndcg = evaluate_metrics(user, K)\n","        if hr is not None:\n","            hr_list.append(hr)\n","        if ndcg is not None:\n","            ndcg_list.append(ndcg)\n","\n","    hr_scores.append(np.mean(hr_list))\n","    ndcg_scores.append(np.mean(ndcg_list))\n","\n","plt.figure(figsize=(10, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(K_values, hr_scores, marker='o', linestyle='-', label=\"HR@K\")\n","plt.xlabel(\"K\")\n","plt.ylabel(\"Hit Rate (HR@K)\")\n","plt.title(\"HR@K Curve\")\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(K_values, ndcg_scores, marker='s', linestyle='-', label=\"NDCG@K\")\n","plt.xlabel(\"K\")\n","plt.ylabel(\"NDCG Score (NDCG@K)\")\n","plt.title(\"NDCG@K Curve\")\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"tf_env","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.21"}},"nbformat":4,"nbformat_minor":0}